# 리스크 지표 개선 작업계획서

**작성일:** 2026-01-15
**목적:** VaR 정확도 향상 및 포트폴리오 구성용 리스크 지표 추가

---

## 1. 현황 및 문제점

### 1.1 현재 VaR 계산 방식

```python
# kr_additional_metrics.py:253-295
# 90일간 일별 수익률의 5th percentile (1일 VaR)
var_95 = PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY daily_return)
```

### 1.2 문제점

| 문제 | 현재 상태 | 영향 |
|------|----------|------|
| 스케일 불일치 | 1일 VaR로 60일 수익률 검증 | 초과율 45~50% (목표 5%) |
| 정규분포 가정 | sqrt(T) 스케일링 | Fat tail 미반영 |
| 정적 변동성 | 단순 표준편차 | 변동성 클러스터링 무시 |
| 단일 기간 | var_95 하나만 계산 | 다양한 투자 기간 대응 불가 |

### 1.3 개선 목표

- VaR 초과율: 49% → 8% 이하
- 기간별 VaR 제공: 5일, 20일, 60일
- 포트폴리오 구성용 지표 추가

---

## 2. Hurst Exponent 기반 VaR 스케일링

### 2.1 Hurst Exponent 개념

```
H > 0.5: Trending (추세 지속) → 리스크가 sqrt(T)보다 빠르게 증가
H = 0.5: Random Walk (정규분포) → 리스크가 sqrt(T)로 증가
H < 0.5: Mean Reverting (평균 회귀) → 리스크가 sqrt(T)보다 느리게 증가

VaR 스케일링: VaR_T = VaR_1 * T^H
```

### 2.2 Hurst Exponent 계산 방법: R/S Analysis

```python
def calculate_hurst_exponent(returns: np.ndarray, min_chunk=8) -> float:
    """
    R/S (Rescaled Range) Analysis로 Hurst Exponent 계산

    Args:
        returns: 일별 수익률 배열 (최소 100일 권장)
        min_chunk: 최소 청크 크기

    Returns:
        H: Hurst exponent (0 < H < 1)
    """
    N = len(returns)
    if N < min_chunk * 4:
        return 0.5  # 데이터 부족 시 기본값

    # 다양한 청크 크기에 대해 R/S 계산
    chunk_sizes = []
    rs_values = []

    for chunk_size in range(min_chunk, N // 4):
        rs_list = []

        for start in range(0, N - chunk_size + 1, chunk_size):
            chunk = returns[start:start + chunk_size]

            # 평균 조정
            mean_adj = chunk - np.mean(chunk)

            # 누적 편차
            cumsum = np.cumsum(mean_adj)

            # Range (R)
            R = np.max(cumsum) - np.min(cumsum)

            # Standard Deviation (S)
            S = np.std(chunk, ddof=1)

            if S > 0:
                rs_list.append(R / S)

        if rs_list:
            chunk_sizes.append(chunk_size)
            rs_values.append(np.mean(rs_list))

    # Log-log 회귀로 H 추정
    # log(R/S) = H * log(n) + c
    log_n = np.log(chunk_sizes)
    log_rs = np.log(rs_values)

    slope, intercept = np.polyfit(log_n, log_rs, 1)
    H = slope

    # H 범위 제한 (0.1 ~ 0.9)
    H = max(0.1, min(0.9, H))

    return round(H, 4)
```

### 2.3 Hurst 기반 VaR 스케일링

```python
def calculate_var_scaled(var_1d: float, target_days: int, hurst: float) -> float:
    """
    Hurst exponent 기반 VaR 스케일링

    Args:
        var_1d: 1일 VaR (음수, %)
        target_days: 목표 기간 (5, 20, 60일)
        hurst: Hurst exponent

    Returns:
        var_Td: T일 VaR (음수, %)
    """
    # VaR_T = VaR_1 * T^H
    scale_factor = target_days ** hurst
    var_Td = var_1d * scale_factor

    return round(var_Td, 2)
```

### 2.4 예시 계산

```
종목 A:
- 1일 VaR = -3.5%
- Hurst = 0.55 (약한 추세)
- 60일 VaR = -3.5% * 60^0.55 = -3.5% * 9.49 = -33.2%

종목 B:
- 1일 VaR = -3.5%
- Hurst = 0.45 (약한 평균회귀)
- 60일 VaR = -3.5% * 60^0.45 = -3.5% * 6.24 = -21.8%

비교 (sqrt(T) 방식):
- 60일 VaR = -3.5% * sqrt(60) = -3.5% * 7.75 = -27.1%
```

---

## 3. VaR 개선 구현 상세

### 3.1 추가 컬럼 (kr_stock_grade 테이블)

```sql
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS hurst_exponent FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_95_ewma FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_95_5d FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_95_20d FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_95_60d FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_99 FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS var_99_60d FLOAT;
```

### 3.2 구현 함수: Hurst Exponent 계산

```python
async def calculate_hurst_exponent(self) -> Optional[float]:
    """
    Hurst Exponent 계산 (R/S Analysis)

    Storage: hurst_exponent FLOAT (0 < H < 1)
    - H > 0.5: 추세 지속 (Trending)
    - H = 0.5: 랜덤워크 (Random Walk)
    - H < 0.5: 평균 회귀 (Mean Reverting)
    """
    try:
        query = """
        WITH daily_returns AS (
            SELECT
                date,
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0)) as daily_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '252 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
            ORDER BY date
        )
        SELECT ARRAY_AGG(daily_return ORDER BY date) as returns
        FROM daily_returns
        WHERE daily_return IS NOT NULL
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or not result[0]['returns']:
            return 0.5  # Default

        returns = np.array([float(r) for r in result[0]['returns']])

        if len(returns) < 60:
            return 0.5  # Insufficient data

        # R/S Analysis
        H = self._rs_analysis(returns)

        logger.info(f"Hurst Exponent: {H:.4f}")
        return round(H, 4)

    except Exception as e:
        logger.error(f"Hurst calculation failed: {e}")
        return 0.5

def _rs_analysis(self, returns: np.ndarray, min_chunk: int = 8) -> float:
    """R/S Analysis implementation"""
    N = len(returns)
    if N < min_chunk * 4:
        return 0.5

    chunk_sizes = []
    rs_values = []

    for chunk_size in range(min_chunk, N // 4):
        rs_list = []

        for start in range(0, N - chunk_size + 1, chunk_size):
            chunk = returns[start:start + chunk_size]
            mean_adj = chunk - np.mean(chunk)
            cumsum = np.cumsum(mean_adj)
            R = np.max(cumsum) - np.min(cumsum)
            S = np.std(chunk, ddof=1)

            if S > 0:
                rs_list.append(R / S)

        if rs_list:
            chunk_sizes.append(chunk_size)
            rs_values.append(np.mean(rs_list))

    if len(chunk_sizes) < 3:
        return 0.5

    log_n = np.log(chunk_sizes)
    log_rs = np.log(rs_values)
    slope, _ = np.polyfit(log_n, log_rs, 1)

    return max(0.1, min(0.9, slope))
```

### 3.3 구현 함수: EWMA VaR

```python
async def calculate_var_95_ewma(self, lambda_param: float = 0.94) -> Optional[float]:
    """
    EWMA (Exponentially Weighted Moving Average) 변동성 기반 VaR

    Storage: var_95_ewma FLOAT (음수, %)

    EWMA는 최근 데이터에 더 높은 가중치를 부여하여
    변동성 클러스터링을 반영함
    """
    try:
        query = """
        WITH daily_returns AS (
            SELECT
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0)) as daily_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '90 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
            ORDER BY date
        )
        SELECT ARRAY_AGG(daily_return ORDER BY date) as returns
        FROM daily_returns
        WHERE daily_return IS NOT NULL
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or not result[0]['returns']:
            return None

        returns = np.array([float(r) for r in result[0]['returns']])

        if len(returns) < 20:
            return None

        # EWMA Variance
        ewma_var = returns[0] ** 2
        for r in returns[1:]:
            ewma_var = lambda_param * ewma_var + (1 - lambda_param) * (r ** 2)

        ewma_vol = np.sqrt(ewma_var) * 100  # Convert to percentage

        # VaR 95% (assuming normal distribution)
        var_95 = -1.645 * ewma_vol

        logger.info(f"VaR 95% (EWMA): {var_95:.2f}%")
        return round(var_95, 2)

    except Exception as e:
        logger.error(f"EWMA VaR calculation failed: {e}")
        return None
```

### 3.4 구현 함수: 기간별 VaR (Hurst 스케일링)

```python
async def calculate_var_95_Td(self, target_days: int) -> Optional[float]:
    """
    Hurst Exponent 기반 T일 VaR 계산

    Args:
        target_days: 목표 기간 (5, 20, 60)

    Storage: var_95_5d, var_95_20d, var_95_60d FLOAT (음수, %)
    """
    try:
        # 1일 VaR 계산 (기존 방식)
        var_1d = await self.calculate_var_95()
        if var_1d is None:
            return None

        # Hurst Exponent
        hurst = await self.calculate_hurst_exponent()
        if hurst is None:
            hurst = 0.5  # Default to sqrt(T) scaling

        # Hurst 스케일링
        scale_factor = target_days ** hurst
        var_Td = var_1d * scale_factor

        logger.info(f"VaR 95% ({target_days}d): {var_Td:.2f}% "
                   f"(1d VaR: {var_1d:.2f}%, H: {hurst:.2f}, scale: {scale_factor:.2f})")

        return round(var_Td, 2)

    except Exception as e:
        logger.error(f"VaR {target_days}d calculation failed: {e}")
        return None
```

---

## 4. 변동성 기반 사이징 지표

### 4.1 추가 컬럼

```sql
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS inv_vol_weight FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS downside_vol FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS vol_percentile FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS atr_20d FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS atr_pct_20d FLOAT;
```

### 4.2 지표 명세

| 지표 | 컬럼명 | 계산식 | 용도 |
|------|--------|--------|------|
| 역변동성 가중치 | `inv_vol_weight` | 1 / volatility_annual | Risk Parity 가중치 |
| 하방 변동성 | `downside_vol` | std(음수 수익률만) * sqrt(252) | Sortino, 하방 리스크 |
| 변동성 백분위 | `vol_percentile` | 전체 종목 대비 변동성 순위 | 상대 비교 |
| ATR 20일 | `atr_20d` | 20일 평균 True Range | 중기 변동성 |
| ATR% 20일 | `atr_pct_20d` | atr_20d / close * 100 | 포지션 사이징 |

### 4.3 구현 함수: 역변동성 가중치

```python
async def calculate_inv_vol_weight(self) -> Optional[float]:
    """
    역변동성 가중치 (Risk Parity용)

    Storage: inv_vol_weight FLOAT

    포트폴리오 프로그램에서 사용:
    weight_i = inv_vol_i / sum(inv_vol_all)
    """
    try:
        vol = await self.calculate_volatility_annual()

        if vol is None or vol <= 0:
            return None

        inv_vol = 1 / vol

        logger.info(f"Inverse Volatility Weight: {inv_vol:.6f} (vol: {vol:.2f}%)")
        return round(inv_vol, 6)

    except Exception as e:
        logger.error(f"Inverse vol weight calculation failed: {e}")
        return None
```

### 4.4 구현 함수: 하방 변동성

```python
async def calculate_downside_vol(self) -> Optional[float]:
    """
    하방 변동성 (Downside Volatility)

    Storage: downside_vol FLOAT (%, annualized)

    음수 수익률만의 표준편차
    Sortino Ratio 계산 및 하방 리스크 평가용
    """
    try:
        query = """
        WITH daily_returns AS (
            SELECT
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0) * 100) as daily_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '90 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
            ORDER BY date
        )
        SELECT
            STDDEV(daily_return) as downside_std,
            COUNT(*) as negative_days
        FROM daily_returns
        WHERE daily_return < 0
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['downside_std'] is None:
            return None

        downside_std = float(result[0]['downside_std'])
        downside_vol_annual = downside_std * np.sqrt(252)

        logger.info(f"Downside Volatility: {downside_vol_annual:.2f}% (annualized)")
        return round(downside_vol_annual, 2)

    except Exception as e:
        logger.error(f"Downside volatility calculation failed: {e}")
        return None
```

### 4.5 구현 함수: 변동성 백분위

```python
async def calculate_vol_percentile(self) -> Optional[float]:
    """
    변동성 백분위 (전체 종목 대비)

    Storage: vol_percentile FLOAT (0-100, 낮을수록 안정적)

    현재 종목의 변동성이 전체 시장에서 몇 % 위치인지
    """
    try:
        query = """
        WITH all_volatilities AS (
            SELECT
                symbol,
                STDDEV(daily_return) * SQRT(252) as annual_vol
            FROM (
                SELECT
                    symbol,
                    ((close - LAG(close) OVER (PARTITION BY symbol ORDER BY date))::NUMERIC /
                     NULLIF(LAG(close) OVER (PARTITION BY symbol ORDER BY date), 0) * 100) as daily_return
                FROM kr_intraday_total
                WHERE date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '90 days'
                    AND date <= COALESCE($2::date, CURRENT_DATE)
            ) sub
            WHERE daily_return IS NOT NULL
            GROUP BY symbol
            HAVING COUNT(*) >= 60
        ),
        ranked AS (
            SELECT
                symbol,
                annual_vol,
                PERCENT_RANK() OVER (ORDER BY annual_vol) * 100 as vol_percentile
            FROM all_volatilities
        )
        SELECT vol_percentile
        FROM ranked
        WHERE symbol = $1
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['vol_percentile'] is None:
            return None

        percentile = float(result[0]['vol_percentile'])

        logger.info(f"Volatility Percentile: {percentile:.1f}% (lower is more stable)")
        return round(percentile, 1)

    except Exception as e:
        logger.error(f"Volatility percentile calculation failed: {e}")
        return None
```

### 4.6 구현 함수: ATR 20일

```python
async def calculate_atr_20d(self) -> Optional[dict]:
    """
    20일 ATR (Average True Range)

    Storage: atr_20d FLOAT, atr_pct_20d FLOAT

    중기 변동성 측정, 포지션 사이징용
    """
    try:
        query = """
        WITH true_ranges AS (
            SELECT
                date,
                GREATEST(
                    high - low,
                    ABS(high - LAG(close) OVER (ORDER BY date)),
                    ABS(low - LAG(close) OVER (ORDER BY date))
                ) as tr,
                close
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '30 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
            ORDER BY date
        )
        SELECT
            AVG(tr) as atr_20d,
            (SELECT close FROM true_ranges ORDER BY date DESC LIMIT 1) as current_close
        FROM (
            SELECT tr FROM true_ranges
            ORDER BY date DESC
            LIMIT 20
        ) recent
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['atr_20d'] is None:
            return None

        atr_20d = float(result[0]['atr_20d'])
        current_close = float(result[0]['current_close'])
        atr_pct_20d = (atr_20d / current_close) * 100

        logger.info(f"ATR 20d: {atr_20d:.0f} ({atr_pct_20d:.2f}%)")

        return {
            'atr_20d': round(atr_20d, 2),
            'atr_pct_20d': round(atr_pct_20d, 2)
        }

    except Exception as e:
        logger.error(f"ATR 20d calculation failed: {e}")
        return None
```

---

## 5. CVaR + Risk Budgeting 지표

### 5.1 추가 컬럼

```sql
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS cvar_99 FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS corr_kospi FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS corr_sector_avg FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS tail_beta FLOAT;
ALTER TABLE kr_stock_grade ADD COLUMN IF NOT EXISTS drawdown_duration_avg FLOAT;
```

### 5.2 지표 명세

| 지표 | 컬럼명 | 계산식 | 용도 |
|------|--------|--------|------|
| CVaR 99% | `cvar_99` | VaR 99% 이하 평균 손실 | 극단 리스크 |
| KOSPI 상관계수 | `corr_kospi` | 60일 rolling correlation | 시장 분산 효과 |
| 섹터 평균 상관 | `corr_sector_avg` | 동일 섹터 종목간 평균 상관 | 집중 리스크 |
| Tail Beta | `tail_beta` | 하락장에서의 Beta | 위기시 민감도 |
| 평균 낙폭 기간 | `drawdown_duration_avg` | 낙폭 회복까지 평균 일수 | 회복력 |

### 5.3 구현 함수: CVaR 99%

```python
async def calculate_cvar_99(self) -> Optional[float]:
    """
    CVaR 99% (Expected Shortfall at 99%)

    Storage: cvar_99 FLOAT (음수, %)

    극단적 상황(하위 1%)에서의 평균 손실
    Basel III 권고 지표
    """
    try:
        query = """
        WITH daily_returns AS (
            SELECT
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0) * 100) as daily_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '252 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
            ORDER BY date
        ),
        var_threshold AS (
            SELECT PERCENTILE_CONT(0.01) WITHIN GROUP (ORDER BY daily_return) as var_99
            FROM daily_returns
            WHERE daily_return IS NOT NULL
        )
        SELECT
            AVG(dr.daily_return) as cvar_99,
            COUNT(*) as tail_count
        FROM daily_returns dr, var_threshold vt
        WHERE dr.daily_return IS NOT NULL
            AND dr.daily_return <= vt.var_99
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['cvar_99'] is None:
            return None

        cvar_99 = float(result[0]['cvar_99'])
        tail_count = int(result[0]['tail_count'])

        logger.info(f"CVaR 99%: {cvar_99:.2f}% (avg of worst {tail_count} days)")
        return round(cvar_99, 2)

    except Exception as e:
        logger.error(f"CVaR 99% calculation failed: {e}")
        return None
```

### 5.4 구현 함수: KOSPI 상관계수

```python
async def calculate_corr_kospi(self) -> Optional[float]:
    """
    KOSPI와의 60일 상관계수

    Storage: corr_kospi FLOAT (-1 ~ 1)

    시장과의 동조성 측정
    낮을수록 분산 효과 좋음
    """
    try:
        query = """
        WITH stock_returns AS (
            SELECT
                date,
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0)) as stock_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '90 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
        ),
        kospi_returns AS (
            SELECT
                date,
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0)) as kospi_return
            FROM kr_index_daily
            WHERE symbol = 'KOSPI'
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '90 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
        )
        SELECT CORR(s.stock_return, k.kospi_return) as correlation
        FROM stock_returns s
        INNER JOIN kospi_returns k ON s.date = k.date
        WHERE s.stock_return IS NOT NULL
            AND k.kospi_return IS NOT NULL
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['correlation'] is None:
            return None

        corr = float(result[0]['correlation'])

        logger.info(f"KOSPI Correlation: {corr:.3f}")
        return round(corr, 3)

    except Exception as e:
        logger.error(f"KOSPI correlation calculation failed: {e}")
        return None
```

### 5.5 구현 함수: Tail Beta

```python
async def calculate_tail_beta(self) -> Optional[float]:
    """
    Tail Beta (하락장에서의 민감도)

    Storage: tail_beta FLOAT

    시장이 하락할 때(하위 10%) 해당 종목의 민감도
    일반 Beta보다 위기 상황 예측에 유용
    """
    try:
        query = """
        WITH returns AS (
            SELECT
                s.date,
                ((s.close - LAG(s.close) OVER (ORDER BY s.date))::NUMERIC /
                 NULLIF(LAG(s.close) OVER (ORDER BY s.date), 0)) as stock_return,
                ((k.close - LAG(k.close) OVER (ORDER BY k.date))::NUMERIC /
                 NULLIF(LAG(k.close) OVER (ORDER BY k.date), 0)) as kospi_return
            FROM kr_intraday_total s
            INNER JOIN kr_index_daily k ON s.date = k.date AND k.symbol = 'KOSPI'
            WHERE s.symbol = $1
                AND s.date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '252 days'
                AND s.date <= COALESCE($2::date, CURRENT_DATE)
        ),
        tail_threshold AS (
            SELECT PERCENTILE_CONT(0.10) WITHIN GROUP (ORDER BY kospi_return) as threshold
            FROM returns
            WHERE kospi_return IS NOT NULL
        )
        SELECT
            REGR_SLOPE(r.stock_return, r.kospi_return) as tail_beta,
            COUNT(*) as tail_days
        FROM returns r, tail_threshold t
        WHERE r.kospi_return <= t.threshold
            AND r.stock_return IS NOT NULL
            AND r.kospi_return IS NOT NULL
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date)

        if not result or result[0]['tail_beta'] is None:
            return None

        tail_beta = float(result[0]['tail_beta'])
        tail_days = int(result[0]['tail_days'])

        logger.info(f"Tail Beta: {tail_beta:.3f} (from {tail_days} down days)")
        return round(tail_beta, 3)

    except Exception as e:
        logger.error(f"Tail Beta calculation failed: {e}")
        return None
```

### 5.6 구현 함수: 섹터 평균 상관

```python
async def calculate_corr_sector_avg(self) -> Optional[float]:
    """
    동일 섹터 종목간 평균 상관계수

    Storage: corr_sector_avg FLOAT (-1 ~ 1)

    섹터 내 종목 집중 리스크 측정
    높을수록 섹터 집중 위험
    """
    try:
        # Get sector
        query_sector = """
        SELECT theme FROM kr_stock_detail WHERE symbol = $1
        """
        sector_result = await self.execute_query(query_sector, self.symbol)

        if not sector_result or not sector_result[0]['theme']:
            return None

        sector = sector_result[0]['theme']

        # Calculate avg correlation with same sector stocks
        query = """
        WITH sector_stocks AS (
            SELECT symbol FROM kr_stock_detail WHERE theme = $3 AND symbol != $1
        ),
        my_returns AS (
            SELECT date,
                ((close - LAG(close) OVER (ORDER BY date))::NUMERIC /
                 NULLIF(LAG(close) OVER (ORDER BY date), 0)) as my_return
            FROM kr_intraday_total
            WHERE symbol = $1
                AND date >= COALESCE($2::date, CURRENT_DATE) - INTERVAL '60 days'
                AND date <= COALESCE($2::date, CURRENT_DATE)
        ),
        correlations AS (
            SELECT CORR(m.my_return,
                ((o.close - LAG(o.close) OVER (PARTITION BY o.symbol ORDER BY o.date))::NUMERIC /
                 NULLIF(LAG(o.close) OVER (PARTITION BY o.symbol ORDER BY o.date), 0))
            ) as corr
            FROM my_returns m
            INNER JOIN kr_intraday_total o ON m.date = o.date
            WHERE o.symbol IN (SELECT symbol FROM sector_stocks)
            GROUP BY o.symbol
            HAVING COUNT(*) >= 40
        )
        SELECT AVG(corr) as avg_corr, COUNT(*) as stock_count
        FROM correlations
        WHERE corr IS NOT NULL
        """

        result = await self.execute_query(query, self.symbol, self.analysis_date, sector)

        if not result or result[0]['avg_corr'] is None:
            return None

        avg_corr = float(result[0]['avg_corr'])
        stock_count = int(result[0]['stock_count'])

        logger.info(f"Sector Avg Correlation: {avg_corr:.3f} (with {stock_count} stocks)")
        return round(avg_corr, 3)

    except Exception as e:
        logger.error(f"Sector correlation calculation failed: {e}")
        return None
```

---

## 6. 포트폴리오 프로그램에서의 활용

### 6.1 지표 조회 및 사용

```python
# 포트폴리오 프로그램 (별도)

async def get_risk_metrics(symbol: str) -> dict:
    """DB에서 리스크 지표 조회"""
    query = """
    SELECT
        volatility_annual,
        var_95_60d,
        cvar_99,
        inv_vol_weight,
        downside_vol,
        corr_kospi,
        corr_sector_avg,
        tail_beta,
        hurst_exponent,
        atr_pct_20d
    FROM kr_stock_grade
    WHERE symbol = $1
    ORDER BY date DESC
    LIMIT 1
    """
    return await db.fetchrow(query, symbol)
```

### 6.2 Risk Parity 포트폴리오 구성

```python
def build_risk_parity_portfolio(stocks: List[dict], risk_budget: float = 0.10) -> dict:
    """
    Risk Parity 포트폴리오 구성

    Args:
        stocks: 종목별 리스크 지표 리스트
        risk_budget: 포트폴리오 총 리스크 예산 (예: 10%)

    Returns:
        dict: {symbol: weight}
    """
    # 1. 역변동성 가중치 합계
    inv_vol_sum = sum(s['inv_vol_weight'] for s in stocks if s['inv_vol_weight'])

    # 2. 초기 가중치 (Risk Parity)
    for s in stocks:
        if s['inv_vol_weight']:
            s['weight_raw'] = s['inv_vol_weight'] / inv_vol_sum
        else:
            s['weight_raw'] = 0

    # 3. 상관행렬 구성 (corr_kospi 기반 근사)
    # 실제로는 종목간 상관계수 필요

    # 4. 포트폴리오 변동성 계산
    weights = np.array([s['weight_raw'] for s in stocks])
    vols = np.array([s['volatility_annual'] / 100 for s in stocks])

    # 단순화: 평균 상관계수 가정
    avg_corr = np.mean([s['corr_kospi'] for s in stocks if s['corr_kospi']])

    # 포트폴리오 분산 (단순화)
    port_var = np.sum((weights * vols) ** 2) + \
               2 * avg_corr * np.sum(np.outer(weights * vols, weights * vols) *
                                     (1 - np.eye(len(weights))))
    port_vol = np.sqrt(port_var)

    # 5. 리스크 예산에 맞게 스케일링
    if port_vol > risk_budget:
        scale = risk_budget / port_vol
        for s in stocks:
            s['weight_final'] = s['weight_raw'] * scale
    else:
        for s in stocks:
            s['weight_final'] = s['weight_raw']

    return {s['symbol']: s['weight_final'] for s in stocks}
```

### 6.3 CVaR 기반 리스크 제한

```python
def apply_cvar_constraint(stocks: List[dict], max_cvar: float = -10.0) -> List[dict]:
    """
    CVaR 기반 종목 필터링

    Args:
        stocks: 후보 종목 리스트
        max_cvar: 허용 최대 CVaR (예: -10%)

    Returns:
        CVaR 조건 충족 종목만 반환
    """
    filtered = []
    for s in stocks:
        if s['cvar_99'] and s['cvar_99'] >= max_cvar:
            filtered.append(s)
        else:
            logger.info(f"Excluded {s['symbol']}: CVaR {s['cvar_99']:.2f}% < {max_cvar}%")

    return filtered
```

---

## 7. 구현 일정 및 체크리스트

### Phase 1: VaR 개선 (3일)

- [ ] Hurst Exponent 계산 함수 구현
- [ ] EWMA VaR 계산 함수 구현
- [ ] var_95_5d, var_95_20d, var_95_60d 계산 함수 구현
- [ ] var_99, var_99_60d 계산 함수 구현
- [ ] DB 컬럼 추가 (ALTER TABLE)
- [ ] kr_main.py에 신규 지표 연동
- [ ] 단위 테스트

### Phase 2: 변동성 사이징 지표 (2일)

- [ ] inv_vol_weight 계산 함수 구현
- [ ] downside_vol 계산 함수 구현
- [ ] vol_percentile 계산 함수 구현
- [ ] atr_20d, atr_pct_20d 계산 함수 구현
- [ ] DB 컬럼 추가
- [ ] kr_main.py에 신규 지표 연동
- [ ] 단위 테스트

### Phase 3: CVaR + Risk Budgeting 지표 (3일)

- [ ] cvar_99 계산 함수 구현
- [ ] corr_kospi 계산 함수 구현
- [ ] corr_sector_avg 계산 함수 구현
- [ ] tail_beta 계산 함수 구현
- [ ] drawdown_duration_avg 계산 함수 구현
- [ ] DB 컬럼 추가
- [ ] kr_main.py에 신규 지표 연동
- [ ] 단위 테스트

### Phase 4: 검증 및 백테스트 (2일)

- [ ] kr_model_analyzer.py 수정 (기간별 VaR 검증)
- [ ] VaR 초과율 재계산
- [ ] 백테스트 결과 분석
- [ ] 문서화

---

## 8. 예상 결과

### VaR 초과율 개선 예상

| 기간 | 현재 초과율 | 개선 후 예상 | 비고 |
|------|------------|-------------|------|
| 3일 | 12.3% | 7~8% | var_95 유지, 스케일링 적용 |
| 30일 | 38.6% | 8~10% | var_95_20d 사용 |
| 60일 | 45.6% | 7~9% | var_95_60d 사용 |
| 90일 | 49.4% | 6~8% | var_95_60d * 1.5^H |

### 포트폴리오 구성 개선 효과

- Risk Parity: 변동성 역가중으로 리스크 균등 배분
- CVaR 필터: 극단 리스크 종목 사전 제외
- 상관계수 활용: 분산 효과 최적화

---

## 9. Rolling Window Out-of-Sample Calibration

### 9.1 문제점: 기존 Calibration 방식의 한계

기존 Calibration 방법들 (Platt Scaling, Isotonic Regression 등)의 문제:

```
과거 전체 데이터로 fitting → 오늘 예측에 적용
     ↓
문제: 오늘 분석 시점에서 미래 데이터는 존재하지 않음
     → 과거 데이터에 overfitting된 모델은 실전에서 무용
     → 백테스트에서만 좋아 보이고 실제 성능은 나빠짐
```

### 9.2 해결책: Rolling Window Out-of-Sample Calibration

**핵심 원리: 절대 미래 데이터를 사용하지 않음**

```
시간축: ----[훈련구간]----[검증구간]----[Gap]----[적용]
        t-365        t-90       t-30          t(오늘)

실제 예시:
- 훈련: 2025-01-15 ~ 2025-10-15 (9개월)
- 검증: 2025-10-15 ~ 2025-12-15 (2개월)
- Gap:  2025-12-15 ~ 2026-01-15 (1개월, 데이터 누수 방지)
- 적용: 2026-01-15 (오늘)
```

### 9.3 구현 구조

```python
async def rolling_calibration(
    self,
    today: date,
    train_months: int = 9,
    valid_months: int = 2,
    gap_days: int = 30
) -> Optional[dict]:
    """
    Rolling Window Out-of-Sample Calibration

    Args:
        today: 적용 날짜 (오늘)
        train_months: 훈련 기간 (개월)
        valid_months: 검증 기간 (개월)
        gap_days: Gap 기간 (일) - 데이터 누수 방지

    Returns:
        dict: {
            'calibrator': fitted calibrator object,
            'valid_score': validation score,
            'is_valid': whether calibration should be applied
        }
    """
    from datetime import timedelta
    from dateutil.relativedelta import relativedelta

    # 1. 기간 설정 (미래 데이터 절대 사용 안함)
    valid_end = today - timedelta(days=gap_days)
    valid_start = valid_end - relativedelta(months=valid_months)
    train_end = valid_start - timedelta(days=1)
    train_start = train_end - relativedelta(months=train_months)

    logger.info(f"Train: {train_start} ~ {train_end}")
    logger.info(f"Valid: {valid_start} ~ {valid_end}")
    logger.info(f"Apply: {today}")

    # 2. 훈련 데이터 조회
    train_data = await self._fetch_calibration_data(train_start, train_end)
    if len(train_data) < 100:
        logger.warning("Insufficient training data")
        return {'calibrator': None, 'valid_score': 0, 'is_valid': False}

    # 3. Calibrator 훈련 (Platt Scaling - 파라미터 2개로 overfitting 위험 낮음)
    calibrator = PlattScaling()
    calibrator.fit(
        train_data['predicted_prob'],
        train_data['actual_outcome']
    )

    # 4. 검증 데이터로 성능 확인
    valid_data = await self._fetch_calibration_data(valid_start, valid_end)
    if len(valid_data) < 50:
        logger.warning("Insufficient validation data")
        return {'calibrator': None, 'valid_score': 0, 'is_valid': False}

    calibrated_probs = calibrator.transform(valid_data['predicted_prob'])

    # 5. Calibration Error 계산 (Brier Score 또는 ECE)
    brier_before = brier_score(valid_data['predicted_prob'], valid_data['actual_outcome'])
    brier_after = brier_score(calibrated_probs, valid_data['actual_outcome'])

    improvement = (brier_before - brier_after) / brier_before * 100

    logger.info(f"Brier Score: {brier_before:.4f} → {brier_after:.4f} ({improvement:+.1f}%)")

    # 6. 검증 성능이 개선되면 적용, 아니면 미적용
    is_valid = brier_after < brier_before * 0.95  # 최소 5% 개선 시 적용

    return {
        'calibrator': calibrator if is_valid else None,
        'valid_score': brier_after,
        'is_valid': is_valid,
        'improvement': improvement
    }
```

### 9.4 Platt Scaling (권장 Calibrator)

```python
class PlattScaling:
    """
    Platt Scaling: 시그모이드 함수로 확률 보정

    P_calibrated = 1 / (1 + exp(A * P_raw + B))

    파라미터가 2개(A, B)뿐이라 overfitting 위험이 낮음
    Isotonic Regression보다 out-of-sample 성능 안정적
    """

    def __init__(self):
        self.A = None
        self.B = None

    def fit(self, probs: np.ndarray, labels: np.ndarray):
        """
        Maximum Likelihood로 A, B 추정
        """
        from scipy.optimize import minimize

        def neg_log_likelihood(params):
            A, B = params
            calibrated = 1 / (1 + np.exp(A * probs + B))
            calibrated = np.clip(calibrated, 1e-10, 1 - 1e-10)

            ll = labels * np.log(calibrated) + (1 - labels) * np.log(1 - calibrated)
            return -np.sum(ll)

        result = minimize(neg_log_likelihood, [0, 0], method='BFGS')
        self.A, self.B = result.x

        return self

    def transform(self, probs: np.ndarray) -> np.ndarray:
        """
        학습된 파라미터로 확률 보정
        """
        if self.A is None:
            return probs

        calibrated = 1 / (1 + np.exp(self.A * probs + self.B))
        return calibrated
```

### 9.5 적용 시 주의사항

| 항목 | 권장사항 | 이유 |
|------|----------|------|
| Gap Period | 30일 이상 | 데이터 누수 완전 차단 |
| 훈련 기간 | 6~12개월 | 충분한 샘플 + 최신 패턴 반영 |
| 검증 기간 | 2~3개월 | 계절성 반영 |
| Calibrator | Platt Scaling | 파라미터 적어 overfitting 위험 낮음 |
| 적용 조건 | 검증 성능 5% 이상 개선 시 | 성능 저하 방지 |
| Fallback | 원본 확률 사용 | 보정이 오히려 나쁠 때 대비 |

### 9.6 일일 자동 업데이트 로직

```python
async def daily_calibration_update(self):
    """
    매일 실행: Rolling Window로 calibrator 업데이트
    """
    today = date.today()

    # 1. 새로운 calibrator 훈련
    result = await self.rolling_calibration(today)

    if result['is_valid']:
        # 2. 유효하면 저장
        await self._save_calibrator(result['calibrator'], today)
        logger.info(f"Calibrator updated: improvement {result['improvement']:.1f}%")
    else:
        # 3. 유효하지 않으면 이전 calibrator 유지 또는 미적용
        logger.info("Calibration skipped: no improvement on validation set")

    # 4. 오늘 예측에 적용
    if result['is_valid']:
        today_probs = await self._get_today_predictions()
        calibrated_probs = result['calibrator'].transform(today_probs)
        await self._save_calibrated_predictions(calibrated_probs, today)
```

### 9.7 왜 이 방법이 최선인가

| 방법 | 장점 | 단점 |
|------|------|------|
| 전체 데이터 Fitting | 백테스트 성능 최고 | 미래 정보 사용 (실전 무용) |
| 고정 과거 Fitting | 미래 정보 없음 | 최신 패턴 미반영 |
| **Rolling Out-of-Sample** | 미래 정보 없음 + 최신 패턴 반영 + 검증 포함 | 구현 복잡도 |

**결론:** 실제 운용 조건 (미래 데이터 없음)에서 유일하게 유효한 방법

### 9.8 구현 일정 추가

#### Phase 5: Rolling Calibration (2일)

- [ ] PlattScaling 클래스 구현
- [ ] rolling_calibration 함수 구현
- [ ] _fetch_calibration_data 쿼리 작성
- [ ] daily_calibration_update 스케줄러 연동
- [ ] 검증 결과 로깅 및 모니터링
- [ ] 단위 테스트

---

## 10. Momentum/Quality 조건부 중립화

### 10.1 문제 배경

90일 IC 분석 결과, 특정 market_state에서 momentum과 quality의 예측력이 없거나 역효과가 발생함.

**90일 IC 분석 요약:**

| Factor | 실패 상태 수 | 전체 샘플 비율 | 문제 |
|--------|-------------|---------------|------|
| Momentum | 3개 | 4.6% | IC 음수 (역방향 예측) |
| Quality | 8개 | 15.5% | IC 음수 (역방향 예측) |

### 10.2 실패 조건 정의

#### Momentum 실패 조건 (3개 상태)

| 조건 | Market State | 90일 IC | 논리 |
|------|--------------|---------|------|
| 1 | "모멘텀형" 포함 | -0.012 | 이미 모멘텀으로 분류된 종목은 추가 모멘텀 효과 없음 |
| 2 | "역발상" 포함 | -0.092 | 역발상 종목에서 순방향 모멘텀은 역효과 |
| 3 | "확장" + "과열" | -0.012 | 과열 시장에서 모멘텀 평균회귀 |

해당 상태:
- KOSPI중형-확장과열-모멘텀형
- 테마특화-모멘텀폭발형
- KOSDAQ소형-침체-극단역발상형

#### Quality 실패 조건 (8개 상태)

| 조건 | 키워드 | 해당 상태 수 | 논리 |
|------|--------|-------------|------|
| 1 | "침체" 또는 "공포" | 4개 | 침체/공포 시장에서 quality 무의미 |
| 2 | "역발상" | 2개 | 역발상 전략에서 quality 역효과 |
| 3 | "탐욕" 또는 "과열" | 3개 | 투기 시장에서 quality 무시됨 |
| 4 | "테마" + "모멘텀폭발" | 1개 | 테마주 급등에서 quality 무관 |

해당 상태:
- 테마특화-모멘텀폭발형 (IC: -0.209)
- KOSDAQ중형-침체공포-역발상형 (IC: -0.161)
- KOSPI중형-둔화공포-혼조형 (IC: -0.094)
- KOSDAQ소형-침체-극단역발상형 (IC: -0.097)
- KOSDAQ소형-성장테마-고위험형 (IC: -0.050)
- KOSPI중형-확장과열-모멘텀형 (IC: -0.035)
- KOSDAQ중형-확장탐욕-공격성장형 (IC: -0.030)
- 기타 (IC: -0.164)

### 10.3 해결 방안: 50점 중립화

**기존 방식의 문제:**

```
방법 1: 가중치를 0으로 설정 후 재분배
→ 문제: growth/value가 과대평가되어 본질과 다른 평가 발생

방법 2: 해당 factor 완전 제외
→ 문제: 100점 만점 스케일이 깨짐
```

**해결책: 50점 강제 (중립화)**

```
조건 충족 시:
momentum_score = 50  (평균)
quality_score = 50   (평균)
```

**논리적 근거:**
- "예측력이 없다" = "모든 종목을 평균으로 취급해야 한다"
- 50점 = 100점 만점에서 정확히 중립
- 가중치 비율 유지, 스케일 유지, 과대평가 방지

### 10.4 검증

```
종목 A: value=80, quality=30, momentum=30, growth=80
종목 B: value=80, quality=70, momentum=70, growth=80

가중치: (0.15, 0.20, 0.15, 0.50)
```

**기존 방식:**

| 종목 | 계산 | 점수 |
|------|------|------|
| A | 0.15×80 + 0.20×30 + 0.15×30 + 0.50×80 | 62.5 |
| B | 0.15×80 + 0.20×70 + 0.15×70 + 0.50×80 | 76.5 |

**50점 중립화 적용 (momentum, quality 실패 조건 충족 시):**

| 종목 | 계산 | 점수 |
|------|------|------|
| A | 0.15×80 + 0.20×50 + 0.15×50 + 0.50×80 | 69.5 |
| B | 0.15×80 + 0.20×50 + 0.15×50 + 0.50×80 | 69.5 |

**효과:**
- 100점 만점 유지: O
- 가중치 비율 유지: O
- 과대/과소평가 방지: O
- 논리적 의미: "예측력 없음 = 모든 종목 평균 취급"

### 10.5 구현 코드

```python
# weight.py - WeightCalculator 클래스에 추가

def get_neutralized_scores(self, market_state: str, scores: dict) -> dict:
    """
    market_state에 따라 예측력 없는 factor를 50점으로 중립화

    Args:
        market_state: 현재 시장 상태
        scores: {'value': float, 'quality': float, 'momentum': float, 'growth': float}

    Returns:
        dict: 중립화 적용된 scores
    """
    neutralized = scores.copy()

    # Momentum neutralization conditions
    momentum_neutralize = False

    if any(x in market_state for x in ['모멘텀형', '모멘텀폭발']):
        momentum_neutralize = True

    if '역발상' in market_state:
        momentum_neutralize = True

    if '확장' in market_state and '과열' in market_state:
        momentum_neutralize = True

    # Quality neutralization conditions
    quality_neutralize = False

    if any(x in market_state for x in ['침체', '공포']):
        quality_neutralize = True

    if '역발상' in market_state:
        quality_neutralize = True

    if '탐욕' in market_state:
        quality_neutralize = True

    if '과열' in market_state and '모멘텀' in market_state:
        quality_neutralize = True

    if '테마' in market_state and '모멘텀폭발' in market_state:
        quality_neutralize = True

    # Apply neutralization
    if momentum_neutralize:
        neutralized['momentum'] = 50.0

    if quality_neutralize:
        neutralized['quality'] = 50.0

    return neutralized
```

**호출 위치: kr_main.py**

```python
# factor score 계산 후, final_score 계산 전에 적용

# 기존 코드
value_score = value_result.get('weighted_score', 0)
quality_score = quality_result.get('weighted_score', 0)
momentum_score = momentum_result.get('weighted_score', 0)
growth_score = growth_result.get('weighted_score', 0)

# 추가할 코드
calculator = WeightCalculator()
scores = {
    'value': value_score,
    'quality': quality_score,
    'momentum': momentum_score,
    'growth': growth_score
}
neutralized_scores = calculator.get_neutralized_scores(market_state, scores)

# 중립화된 점수로 final_score 계산
base_factor_score = (
    neutralized_scores['value'] * final_weights['value'] +
    neutralized_scores['quality'] * final_weights['quality'] +
    neutralized_scores['momentum'] * final_weights['momentum'] +
    neutralized_scores['growth'] * final_weights['growth']
)
```

### 10.6 구현 일정

#### Phase 6: Momentum/Quality 조건부 중립화 (1일)

- [ ] `get_neutralized_scores()` 함수 구현 (weight.py)
- [ ] kr_main.py에서 중립화 함수 호출 추가
- [ ] 로깅 추가 (중립화 적용 시 로그 출력)
- [ ] 단위 테스트
- [ ] 백테스트로 효과 검증

---

## 11. 기존 데이터 신규 컬럼 일괄 계산 스크립트

### 11.1 필요성

kr_stock_grade 테이블에 신규 컬럼이 추가되었으나, 기존 레코드는 해당 컬럼값이 NULL 상태임.
기존 데이터에 대해 신규 컬럼만 별도로 계산하여 UPDATE하는 스크립트 필요.

### 11.2 신규 추가 컬럼 목록 (17개)

**Phase 1: VaR 개선 (7개)**
| 컬럼명 | 설명 |
|--------|------|
| hurst_exponent | Hurst 지수 (0-1): 추세지속/평균회귀 판단 |
| var_95_ewma | EWMA VaR 95%: 최근 변동성 가중 |
| var_95_5d | 5일 보유 VaR 95% |
| var_95_20d | 20일 보유 VaR 95% |
| var_95_60d | 60일 보유 VaR 95% |
| var_99 | 일일 VaR 99% |
| var_99_60d | 60일 보유 VaR 99% |

**Phase 2: 변동성 사이징 (5개)**
| 컬럼명 | 설명 |
|--------|------|
| inv_vol_weight | 역변동성 가중치 |
| downside_vol | 하방 변동성 (연환산) |
| vol_percentile | 변동성 백분위수 |
| atr_20d | ATR 20일 (원화) |
| atr_pct_20d | ATR% 20일 |

**Phase 3: CVaR + Risk Budgeting (5개)**
| 컬럼명 | 설명 |
|--------|------|
| cvar_99 | CVaR 99% (Expected Shortfall) |
| corr_kospi | KOSPI 상관계수 |
| corr_sector_avg | 섹터 평균 상관계수 |
| tail_beta | 하락장 베타 |
| drawdown_duration_avg | 평균 낙폭 지속일 |

### 11.3 스크립트 구조

```python
# kr/backfill_new_columns.py

"""
기존 kr_stock_grade 레코드에 대해 신규 컬럼만 계산하여 UPDATE하는 스크립트

사용법:
    python backfill_new_columns.py --start-date 2025-08-01 --end-date 2026-01-15
    python backfill_new_columns.py --symbol 005930  # 특정 종목만
    python backfill_new_columns.py --batch-size 1000  # 배치 크기 조절
"""

import asyncio
import asyncpg
import argparse
from datetime import date, timedelta
from typing import Optional, List
import logging

logger = logging.getLogger(__name__)


class BackfillNewColumns:
    """신규 컬럼 일괄 계산 클래스"""

    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool

    async def get_target_records(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        symbol: Optional[str] = None,
        batch_size: int = 1000,
        offset: int = 0
    ) -> List[dict]:
        """
        UPDATE 대상 레코드 조회 (신규 컬럼이 NULL인 레코드)
        """
        query = """
        SELECT symbol, date
        FROM kr_stock_grade
        WHERE hurst_exponent IS NULL
          AND ($1::date IS NULL OR date >= $1)
          AND ($2::date IS NULL OR date <= $2)
          AND ($3::text IS NULL OR symbol = $3)
        ORDER BY date, symbol
        LIMIT $4 OFFSET $5
        """
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(query, start_date, end_date, symbol, batch_size, offset)
            return [dict(row) for row in rows]

    async def calculate_new_columns(self, symbol: str, analysis_date: date) -> dict:
        """
        단일 레코드에 대한 신규 컬럼 계산

        Returns:
            dict: {column_name: value} 형태
        """
        result = {}

        # Phase 1: VaR 개선
        result['hurst_exponent'] = await self._calc_hurst_exponent(symbol, analysis_date)
        result['var_95_ewma'] = await self._calc_var_95_ewma(symbol, analysis_date)

        var_1d = await self._calc_var_1d(symbol, analysis_date)
        hurst = result['hurst_exponent'] or 0.5

        result['var_95_5d'] = self._scale_var(var_1d, 5, hurst) if var_1d else None
        result['var_95_20d'] = self._scale_var(var_1d, 20, hurst) if var_1d else None
        result['var_95_60d'] = self._scale_var(var_1d, 60, hurst) if var_1d else None

        result['var_99'] = await self._calc_var_99(symbol, analysis_date)
        result['var_99_60d'] = self._scale_var(result['var_99'], 60, hurst) if result['var_99'] else None

        # Phase 2: 변동성 사이징
        result['inv_vol_weight'] = await self._calc_inv_vol_weight(symbol, analysis_date)
        result['downside_vol'] = await self._calc_downside_vol(symbol, analysis_date)
        result['vol_percentile'] = await self._calc_vol_percentile(symbol, analysis_date)

        atr_result = await self._calc_atr_20d(symbol, analysis_date)
        result['atr_20d'] = atr_result.get('atr_20d') if atr_result else None
        result['atr_pct_20d'] = atr_result.get('atr_pct_20d') if atr_result else None

        # Phase 3: CVaR + Risk Budgeting
        result['cvar_99'] = await self._calc_cvar_99(symbol, analysis_date)
        result['corr_kospi'] = await self._calc_corr_kospi(symbol, analysis_date)
        result['corr_sector_avg'] = await self._calc_corr_sector_avg(symbol, analysis_date)
        result['tail_beta'] = await self._calc_tail_beta(symbol, analysis_date)
        result['drawdown_duration_avg'] = await self._calc_drawdown_duration_avg(symbol, analysis_date)

        return result

    async def update_record(self, symbol: str, analysis_date: date, columns: dict) -> bool:
        """
        단일 레코드 UPDATE
        """
        # NULL이 아닌 컬럼만 UPDATE
        non_null_cols = {k: v for k, v in columns.items() if v is not None}

        if not non_null_cols:
            return False

        set_clause = ', '.join([f"{k} = ${i+3}" for i, k in enumerate(non_null_cols.keys())])
        values = list(non_null_cols.values())

        query = f"""
        UPDATE kr_stock_grade
        SET {set_clause}
        WHERE symbol = $1 AND date = $2
        """

        async with self.db_pool.acquire() as conn:
            await conn.execute(query, symbol, analysis_date, *values)

        return True

    async def run(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        symbol: Optional[str] = None,
        batch_size: int = 1000
    ):
        """
        메인 실행 함수
        """
        offset = 0
        total_updated = 0
        total_failed = 0

        while True:
            records = await self.get_target_records(
                start_date, end_date, symbol, batch_size, offset
            )

            if not records:
                break

            logger.info(f"Processing batch: offset={offset}, count={len(records)}")

            for record in records:
                try:
                    columns = await self.calculate_new_columns(
                        record['symbol'],
                        record['date']
                    )
                    success = await self.update_record(
                        record['symbol'],
                        record['date'],
                        columns
                    )
                    if success:
                        total_updated += 1
                    else:
                        total_failed += 1

                except Exception as e:
                    logger.error(f"Failed {record['symbol']} {record['date']}: {e}")
                    total_failed += 1

            offset += batch_size
            logger.info(f"Progress: updated={total_updated}, failed={total_failed}")

        logger.info(f"Completed: total_updated={total_updated}, total_failed={total_failed}")


async def main():
    parser = argparse.ArgumentParser(description='Backfill new columns in kr_stock_grade')
    parser.add_argument('--start-date', type=str, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='End date (YYYY-MM-DD)')
    parser.add_argument('--symbol', type=str, help='Specific symbol to process')
    parser.add_argument('--batch-size', type=int, default=1000, help='Batch size')

    args = parser.parse_args()

    start_date = date.fromisoformat(args.start_date) if args.start_date else None
    end_date = date.fromisoformat(args.end_date) if args.end_date else None

    # DB connection
    from dotenv import load_dotenv
    import os
    load_dotenv()

    db_url = os.getenv('DATABASE_URL').replace('postgresql+asyncpg://', 'postgresql://')
    pool = await asyncpg.create_pool(db_url)

    try:
        backfiller = BackfillNewColumns(pool)
        await backfiller.run(
            start_date=start_date,
            end_date=end_date,
            symbol=args.symbol,
            batch_size=args.batch_size
        )
    finally:
        await pool.close()


if __name__ == '__main__':
    asyncio.run(main())
```

### 11.4 구현 일정

#### Phase 7: 기존 데이터 신규 컬럼 Backfill 스크립트

- [ ] `backfill_new_columns.py` 파일 생성
- [ ] Phase 1~3 개별 계산 함수 구현 (각 Phase 완료 후 추가)
- [ ] 배치 처리 로직 구현
- [ ] 진행률 로깅 및 에러 처리
- [ ] 테스트 실행 (소량 데이터)
- [ ] 전체 데이터 Backfill 실행

### 11.5 실행 순서

```
1. Phase 1~3 계산 함수 구현 완료
2. DB 컬럼 추가 (ALTER TABLE)
3. backfill_new_columns.py 스크립트 생성
4. 테스트 실행: python backfill_new_columns.py --symbol 005930
5. 전체 실행: python backfill_new_columns.py --start-date 2025-08-01 --end-date 2026-01-15
6. kr_main.py 연동 (신규 데이터는 자동 계산)
```

---

**작성 완료**
